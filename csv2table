#!/usr/bin/env python

import os
import re
import csv
import sys
import gzip
import decimal
import datetime
import argparse
import urlparse

TEXT = "text"
INTEGER = "integer"
NUMERIC = "numeric"
DATETIME = "timestamp"
DATETIMETZ = "timestamptz"
BIGINTEGER = "biginteger"


parser = argparse.ArgumentParser()
parser.add_argument("--file", "-f", help="csv file", required=True)
parser.add_argument("--copy", "-y", help="issue a copy statement, for import",
                    action="store_true", default=False)
parser.add_argument("--backslash", "-s", help="when issuing copy, use \copy",
                    action="store_true", default=False)
parser.add_argument("--delimiter", "-d", help="csv delimiter", default=",")
parser.add_argument("--quote", "-q", help="csv quote", default='"')
parser.add_argument("--table", "-t", help="table name", default=None)
parser.add_argument("--schema", help="schema name", default=None)
parser.add_argument("--create-schema", help="create the named schema",
                    action="store_true", default=False)
parser.add_argument("--integer", "-i", help="integer columns",
                    action="store_true", default=False)
parser.add_argument("--numeric", "-n", help="numeric parsing",
                    action="store_true", default=False)
parser.add_argument("--timestamp", "-p", help="timestamp parsing",
                    action="store_true", default=False)
parser.add_argument("--tz", help="use timestamptz instead of timestamp",
                    action="store_true", default=False)
parser.add_argument("--mogrify", "-m", help="clean names",
                    action="store_true", default=False)
parser.add_argument("--lower", "-l", help="lowercase names",
                    action="store_true", default=False)
parser.add_argument("--drop", "-x", help="drop table first",
                    action="store_true", default=False)
parser.add_argument("--skip-parsing", help="skip type parsing for cols")
parser.add_argument("--big-integer", "-b", help="use bigint instead of int",
                    default=False, action="store_true")
parser.add_argument("--no-create", "-z", help="don't issue create table",
                    default=False, action="store_true")
parser.add_argument("--fix-duplicates",
                    help=("handle duplicate column names by appending an "
                          "increasing suffix"),
                    default=False, action="store_true")
parser.add_argument("--transaction", "-1", help="run in a transaction",
                    default=False, action="store_true")
parser.add_argument("--temporary", help="create a temporary table",
                    default=False, action="store_true")
parser.add_argument("--file-column",
                    help=("add a column to the created table which will contain "
                          "the name of the file the data was loaded from"),
                    default=None)
parser.add_argument("--redshift",
                    help=("generate a copy command for redshift. takes an "
                          "optional argument, a file to look for s3 "
                          "credentials/bucket in. To try and read from the "
                          "environment, specify <ENV>"),
                    default=None)
parser.add_argument("--redshift-bucket",
                    help=("when generating a copy command for redshift, using "
                          "this option specifies the bucket to store the file "
                          "in, if requested. This will override any bucket found "
                          "by the --redshift option."),
                    default=None)
parser.add_argument("--redshift-upload",
                    help="upload the specified file to redshift",
                    action="store_true", default=False)
parser.add_argument("--gzip", help="the file is in gzip format",
                    action="store_true", default=False)
parser.add_argument("--missing-headers",
                     help="file is missing headers, make up column names",
                     action="store_true", default=False)

args = parser.parse_args()

# check that our given file exists
if not args.file or not os.path.exists(args.file):
    sys.stderr.write("file '{}' does not exist.\n".format(args.file))
    sys.exit(1)

# check that a schema was given if create schema was requested
if args.create_schema and args.schema is None:
    sys.stderr.write("--create-schema given but --schema not provided.\n")
    sys.exit(1)

# if --redshift-upload is specified, and --redshift is not set, force it to
# "<ENV>"
if args.redshift_upload and not args.redshift:
    args.redshift = "<ENV>"

# --backslash is incompatible with --redshift
if args.backslash and args.redshift:
    sys.stderr.write("--backslash is incompatible with --redshift*")
    sys.exit(1)

# name cleaner function
empties = ""
def clean_name(name):
    global empties
    name = name.strip()
    name = re.sub(r'[^a-zA-Z0-9_]', "_", name)
    name = re.sub(r'_+', "_", name)
    if args.lower:
        name = name.lower()
    if not name:
        empties += "_"
        name = empties + ""
    return name

# construct a table name from the file name
if args.table is None:
    path_parts = os.path.split(args.file)
    file_name = path_parts[1]

    pieces = file_name.split(".")
    if len(pieces) > 1:
        if pieces[-1] == "gz":
            pieces.pop()
        pieces = pieces[:-1]

    table_name = ".".join(pieces)

    if args.mogrify:
        table_name = clean_name(table_name)
else:
    table_name = args.table

# determine any columns we should not type parse
skip_type_parsing = []
if args.skip_parsing:
    cols = args.skip_parsing.split(",")
    skip_type_parsing = [c.strip() for c in cols]

# get a handle on things
if args.gzip:
    fp = gzip.open(args.file, "rU")
    fp._read_eof = lambda *a, **k: None
else:
    fp = open(args.file, "rU")

# try and figure out the type of the given value
def get_type(value, args):
    # date formats will be crossed with time formats
    date_formats = [
        "%Y-%m-%d",
        "%Y%m%d",
        "%m/%d/%Y",
        "%b %d, %Y",
        "%m/%d/%y",
    ]

    time_formats = [
        "%H:%M:%S",
        "%H:%M:%S %z",
        "%H:%M:%S %Z",
        "%I:%M:%S %p",
        "%I:%M:%S %P",
        "%H:%M",
    ]

    datetime_formats = []
    for date_format in date_formats:
        datetime_formats.append(date_format)
        for time_format in time_formats:
            datetime_formats.append(date_format + " " + time_format)
            datetime_formats.append(date_format + "T" + time_format)

    # first the timestamp
    if args.timestamp:
        for timestamp_format in datetime_formats:
            try:
                datetime.datetime.strptime(value, timestamp_format)
                if not (len(value) != 8 and timestamp_format == "%Y%m%d"):
                    if args.tz:
                        return DATETIMETZ
                    else:
                        return DATETIME
            except ValueError:
                pass

    # then integers
    if args.integer:
        try:
            int(value)
            if args.big_integer:
                return BIGINTEGER
            else:
                return INTEGER
        except ValueError:
            pass

    # then numeric
    if args.numeric:
        try:
            decimal.Decimal(value)
            return NUMERIC
        except decimal.InvalidOperation:
            pass
    
    # we got nothing
    return TEXT

# some help to pass tabs
delimiter = args.delimiter
if delimiter in ("\\t", "\\\\t"):
    delimiter = "\t"

# start processing the file
first = True
header_mapping = {}
reader = csv.DictReader(fp, delimiter=delimiter, quotechar=args.quote)

# figure out the column name and types
seen_column_names = {}

# if a file name column was requested, pre-seed that name here
if args.file_column is not None:
    seen_column_names[args.file_column] = 0

for record in reader:
    fields = reader.fieldnames

    if args.mogrify:
        for field in fields:
            header_mapping[field] = clean_name(field)
    else:
        # strip quotes to work with less chance of failure.
        header_mapping = {f: f.replace('"', "") for f in fields}

    # build a table with some hopefully reasonable types
    columns = []
    for i, field in enumerate(fields):
        value = record[field]

        # typed columns (with maybe nice names)
        if field in skip_type_parsing \
                or header_mapping[field] in skip_type_parsing:
            column_type = TEXT
        else:
            column_type = get_type(value, args)

        if args.missing_headers:
            column_name = "col_{}".format(i)
        else:
            column_name = header_mapping[field]
            if args.fix_duplicates:
                if column_name in seen_column_names:
                    original_name = column_name
                    suffix = seen_column_names[original_name] + 1
                    column_name = "{}_{}".format(original_name, suffix)
                    seen_column_names[original_name] = suffix
                else:
                    seen_column_names[column_name] = 0

        column = (column_name, column_type)
        columns.append(column)

    # enough of this boring crap
    break

# if redshift was requested, attempt to figure out our credentials and bucket.
# note that we'll potentially need access to the credentials below, so define
# some variables up here
aws_access_key_id = None
aws_secret_access_key = None
s3_path = None

if args.redshift:

    def read_redshift_file(path):
        aws_access_key_id = None
        aws_secret_access_key = None
        s3_bucket = None
        s3_transfer_bucket = None
        
        if os.path.exists(path):

            # open and "parse" file. because this file can actually contain
            # no headers, we can't really use ConfigParser
            with open(path, "r") as rfp:
                for line in rfp.readlines():
                    if line.startswith("#"):
                        continue

                    id_match = re.search(r'(AWSAccessKeyId|aws_access_key_id|access_key|s3_account_id)\s*=\s*(.+)$', line)
                    if id_match:
                        aws_access_key_id = id_match.groups()[1].strip()

                    key_match = re.search(r'(AWSSecretKey|aws_secret_access_key|secret_key|s3_private_key)\s*=\s*(.+)$', line)
                    if key_match:
                        aws_secret_access_key = key_match.groups()[1].strip()

                    bucket_match = re.search(r's3_bucket\s*=\s*(.+)$', line)
                    if bucket_match:
                        s3_bucket = bucket_match.groups()[0].strip()

                    transfer_match = re.search(r's3_transfer_bucket\s*=\s*(.+)$', line)
                    if transfer_match:
                        s3_transfer_bucket = transfer_match.groups()[0].strip()

        return aws_access_key_id, aws_secret_access_key, (s3_transfer_bucket or s3_bucket)

    if args.redshift == "<ENV>":
        aws_access_key_id = os.getenv("AWS_ACCESS_KEY_ID")
        aws_secret_access_key = os.getenv("AWS_SECRET_ACCESS_KEY")
        s3_bucket = os.getenv("S3_BUCKET")

        # maybe we didn't find a key, look for a credentials specified in the
        # env
        if aws_access_key_id is None:
            credentials_path = os.getenv("AWS_CREDENTIAL_FILE")
            maybe_key_id, maybe_key, maybe_bucket = read_redshift_file(credentials_path)
            if aws_access_key_id is None:
                aws_access_key_id = maybe_key_id
            if aws_secret_access_key is None:
                aws_secret_access_key = maybe_key
            if s3_bucket is None:
                s3_bucket = maybe_bucket
    else:
        aws_access_key_id, aws_secret_access_key, s3_bucket = read_redshift_file(args.redshift)

    if args.redshift_bucket:
        s3_bucket = args.redshift_bucket

    # we need to have access credentials and a bucket, abort if not found
    if aws_access_key_id is None:
        sys.stderr.write("unable to determine aws_access_key_id")
        sys.exit(1)

    if aws_secret_access_key is None:
        sys.stderr.write("unable to determine aws_secret_access_key")
        sys.exit(1)

    if s3_bucket is None:
        sys.stderr.write("unable to determine s3_bucket")
        sys.exit(1)

    # now, build a url from our components. we will then parse this url to get
    # our bucket and key name components, because the bucket may have been
    # specified with a prefix
    s3_path = "s3://" + os.path.join(s3_bucket, os.path.basename(args.file))
    parsed = urlparse.urlparse(s3_path)
    real_bucket = parsed.netloc
    real_key = parsed.path.lstrip("/")

    # if redshift upload was requested, attempt to upload the file
    if args.redshift_upload:

        # we import boto here so it's not always required
        from boto.s3.connection import S3Connection

        # connect to s3
        conn = S3Connection(aws_access_key_id, aws_secret_access_key)

        # get bukkit
        bucket = conn.get_bucket(real_bucket)

        # make key, put file
        key = bucket.new_key(real_key)
        key.set_contents_from_filename(args.file)

# generate our name clause
name_clause = ""
if args.schema:
    name_clause += '"{}".'.format(args.schema)
name_clause += '"{}"'.format(table_name)

# generate our columns clause for table create
columns_fragments = []
for column_name, column_type in columns:
    columns_fragments.append('    "{}" {}'.format(column_name, column_type))

# if we requested a column containing the filename, add that in
if args.file_column is not None:
    file_column_frag = ("    \"{}\" text default '{}'"
                        .format(args.file_column, args.file))
    columns_fragments = [file_column_frag] + columns_fragments

columns_clause = ",\n".join(columns_fragments)

# generate the target clause to emit with the copy statement
targets_clause = ", ".join(['"{}"'.format(c[0]) for c in columns])

# maybe print a begin
if args.transaction:
    print("begin;")

# maybe print our a schema create
if args.create_schema:
    print('create schema "{}";'.format(args.schema))

# maybe print out a drop
if args.drop and not args.no_create:
    print('drop table if exists {};'.format(name_clause))

# print out our table create statement
if not args.no_create:
    table_clause = "create table"
    if args.temporary:
        table_clause = "create temporary table"
    print("{} {} (".format(table_clause, name_clause))
    print(columns_clause)
    print(");")

# print out our copy statement
if args.copy:
    
    # generate our options clause
    if args.redshift:
        options = ("'{}' with credentials 'aws_access_key_id={};aws_secret_access_key={}' "
                   "csv ignoreheader 1 delimiter '{}' quote '{}'"
                   .format(s3_path, aws_access_key_id, aws_secret_access_key,
                           delimiter, args.quote))
        if args.gzip:
            options += " gzip"
    else:
        if args.gzip:
            start = "program 'gunzip < {}'".format(args.file)
        else:
            start = "'{}'".format(args.file)
        options = start + (" with csv header delimiter '{}' quote '{}'"
                           .format(delimiter, args.quote))

    copy = "copy"
    if args.backslash:
        copy = r'\copy'

    print('{} {}({}) from {};'.format(copy, name_clause, targets_clause, options))

# maybe print out a commit
if args.transaction:
    print("commit;")

